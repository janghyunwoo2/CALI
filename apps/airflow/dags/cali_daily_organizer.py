import os
import re
from datetime import datetime
from airflow import DAG
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.operators.python import PythonOperator

# [Git-sync ìµœì í™” í¬ì¸íŠ¸ 1] 
# Git-syncëŠ” ì½”ë“œë¥¼ íŠ¹ì • ê²½ë¡œ(/dags/...)ì— ë¿Œë ¤ì£¼ê¸° ë•Œë¬¸ì— 
# ë²„í‚·ëª… ê°™ì€ ì„¤ì •ê°’ì€ OS í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ë˜, ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ê¸°ë³¸ê°’ì„ ì£¼ëŠ” ê²Œ ì¢‹ì•„.
BUCKET_NAME = os.getenv('S3_BACKUP_BUCKET', 'cali-log-bucket') 
LANDING_ZONE = 'raw/'      
VAULT_ZONE = 'vault/'      

default_args = {
    'owner': 'cali_admin',
    'start_date': datetime(2026, 1, 1),
    'retries': 1,
}

with DAG(
    dag_id='cali_daily_organizer',
    default_args=default_args,
    schedule_interval='@daily', 
    catchup=False,
    tags=['cali', 'etl', 'classification', 'gitsync'] # Git-sync ê´€ë¦¬ í‘œì‹œ ì¶”ê°€
) as dag:

    def organize_logs_to_vault(**context):
        s3_hook = S3Hook(aws_conn_id='aws_default')
        
        # [Git-sync ìµœì í™” í¬ì¸íŠ¸ 2] 
        # íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ë•Œ í´ë” ê²½ë¡œ ìžì²´(`raw/`)ê°€ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ì—ëŸ¬ê°€ ë‚˜ëŠ” ê²½ìš°ê°€ ë§Žì•„.
        # ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ í•„í„°ë§ì„ ë” ê¼¼ê¼¼í•˜ê²Œ í–ˆì–´.
        all_keys = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=LANDING_ZONE)
        
        if not all_keys:
            print("ðŸ“¢ í˜„ìž¬ ì²˜ë¦¬í•  ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        clean_keys = [k for k in all_keys if k != LANDING_ZONE and not k.endswith('/')]
        
        if not clean_keys:
            print("ðŸ“¢ ì²˜ë¦¬í•  ì§„ì§œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        for key in clean_keys:
            content = s3_hook.read_key(key, BUCKET_NAME)
            
            # ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ
            service_match = re.search(r'\[(.*?)\]', content)
            service_name = service_match.group(1) if service_match else "unknown_service"
            
            # ë¡œê·¸ ë ˆë²¨ íŒŒì•…
            log_level = "ERROR" if "ERROR" in content.upper() else "INFO"
            
            # ë‚ ì§œ íŒŒí‹°ì…”ë‹
            date_partition = datetime.now().strftime('%Y-%m-%d')
            
            # ì‹ ê·œ ê²½ë¡œ ìƒì„±
            filename = key.split('/')[-1]
            new_key = f"{VAULT_ZONE}{service_name}/{log_level}/{date_partition}/{filename}"
            
            # S3 ì´ë™ (Copy & Delete)
            # Git-sync í™˜ê²½ì—ì„œë„ S3 Hookì€ ë™ì¼í•˜ê²Œ ìž‘ë™í•´!
            s3_hook.copy_object(
                source_bucket_key=key, 
                dest_bucket_key=new_key,
                source_bucket_name=BUCKET_NAME, 
                dest_bucket_name=BUCKET_NAME
            )
            s3_hook.delete_objects(bucket=BUCKET_NAME, keys=key)
            
            print(f"âœ… [Git-sync Deploy] ì´ë™ ì™„ë£Œ: {key} -> {new_key}")

    organize_task = PythonOperator(
        task_id='classify_and_move_logs',
        python_callable=organize_logs_to_vault
    )

    organize_task