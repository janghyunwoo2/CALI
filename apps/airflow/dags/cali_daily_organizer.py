import os
import json
import requests
import gzip
import sys
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.operators.python import PythonOperator
from airflow.models import Variable

# EKS í™˜ê²½ì—ì„œ Boto3ì˜ ë‚´ë¶€ í˜¸ì¶œ ë£¨í”„ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¬ê·€ ì œí•œì„ ëŠ˜ë¦¼
sys.setrecursionlimit(3000)

# --- [1. ì„¤ì •] ---
BUCKET_NAME = os.getenv('S3_BACKUP_BUCKET') or "cali-logs-827913617635"
LANDING_ZONE = 'raw/'
STATS_ZONE = 'daily_stats/'
# [ì¶”ê°€] ë¦¬ì „ ëª…ì‹œëŠ” EKS í™˜ê²½ì—ì„œ ì—”ë“œí¬ì¸íŠ¸ íƒìƒ‰ ë£¨í”„ë¥¼ ë§‰ëŠ” í•µì‹¬ì´ì•¼!
AWS_REGION = "ap-northeast-2" 

default_args = {
    'owner': 'cali_admin',
    'start_date': datetime(2026, 1, 28),
    'retries': 1,
    'retry_delay': timedelta(seconds=10),
}

# --- [2. ì—ëŸ¬ ë¶„ë¥˜ê¸°] ---
def classify_error_type(content):
    c = content.lower()
    if any(k in c for k in ['db_cache', 'db_issue', 'database']): return 'Database'
    if any(k in c for k in ['infra', 'eks', 'kubernetes']): return 'Infra_EKS'
    if any(k in c for k in ['payment', 'pg']): return 'Payment'
    if any(k in c for k in ['auth', 'security']): return 'Auth_Security'
    if any(k in c for k in ['business', 'logic', 'microservice']): return 'BusinessLogic'
    return 'Other_Errors'

# --- [3. ìŠ¬ë™ ì „ì†¡ í•¨ìˆ˜ (EKS ì•ˆì •í™” ë²„ì „)] ---
def send_slack_report(date_str, total, errors, counts):
    # EKSì—ì„œëŠ” ì „ì—­ ë³€ìˆ˜ë³´ë‹¤ í•¨ìˆ˜ ì‹¤í–‰ ì‹œì ì— ì§ì ‘ ê°€ì ¸ì˜¤ëŠ” ê²Œ ì œì¼ í™•ì‹¤í•´
    webhook_url = Variable.get("SLACK_WEBHOOK_URL", default_var=os.getenv('SLACK_WEBHOOK_URL'))
    
    if not webhook_url:
        print("âš ï¸ SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    detail_msg = "\n".join([f"â€¢ {cat}: {cnt}ê±´" for cat, cnt in counts.items() if cnt > 0])
    if not detail_msg: detail_msg = "â€¢ íƒì§€ëœ íŠ¹ì´ ì—ëŸ¬ ì—†ìŒ"
    
    payload = {
        "text": f"ğŸ“… *{date_str} Cali ì‹œìŠ¤í…œ ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸ (EKS)*",
        "attachments": [{
            "color": "#FF0000" if errors > 0 else "#36a64f",
            "blocks": [
                {
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": f"*ğŸ“Š ì „ì²´ í†µê³„*\nâ€¢ ì´ ë¡œê·¸: {total}ê±´\nâ€¢ ì—ëŸ¬ íƒì§€: {errors}ê±´"}
                },
                {
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": f"*ğŸš¨ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸*\n{detail_msg}"}
                }
            ]
        }]
    }
    
    try:
        response = requests.post(webhook_url, data=json.dumps(payload), headers={'Content-Type': 'application/json'})
        if response.status_code == 200:
            print("âœ… ìŠ¬ë™ ë¦¬í¬íŠ¸ ì „ì†¡ ì„±ê³µ!")
        else:
            print(f"âŒ ìŠ¬ë™ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
    except Exception as e:
        print(f"âŒ ìŠ¬ë™ ì˜ˆì™¸ ë°œìƒ: {e}")

# --- [4. ë©”ì¸ ë¶„ì„ ë¡œì§] ---
def daily_analysis_and_slack(**context):
    # [í•µì‹¬ ìˆ˜ì •] aws_conn_id=None ì„¤ì •ìœ¼ë¡œ í¬ë“œì˜ IAM Roleì„ ì§ì ‘ ì‚¬ìš©í•˜ê²Œ í•¨
    s3_hook = S3Hook(aws_conn_id=None, region_name=AWS_REGION)
    
    all_keys = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=LANDING_ZONE)
    if not all_keys:
        print("ğŸ“¢ ë¶„ì„í•  ë¡œê·¸ê°€ S3ì— ì—†ìŠµë‹ˆë‹¤.")
        return

    clean_keys = [k for k in all_keys if k != LANDING_ZONE and not k.endswith('/')]
    
    category_counts = {
        "Database": 0, "Infra_EKS": 0, "Payment": 0, 
        "Auth_Security": 0, "BusinessLogic": 0, "Other_Errors": 0
    }
    total_logs = 0
    total_errors = 0
    date_str = datetime.now().strftime('%Y-%m-%d')

    for key in clean_keys:
        # EKS ë‚´ë¶€ì—ì„œëŠ” get_key í˜¸ì¶œ ì‹œ ë¦¬ì†ŒìŠ¤ ìµœì í™”ë¥¼ ìœ„í•´ ì§ì ‘ ë°”ì´íŠ¸ë¥¼ ê¸ì–´ì˜´
        file_obj = s3_hook.get_key(key, BUCKET_NAME)
        raw_content = file_obj.get()['Body'].read()
        total_logs += 1

        try:
            if raw_content.startswith(b'\x1f\x8b'):
                content = gzip.decompress(raw_content).decode('utf-8')
            else:
                content = raw_content.decode('utf-8')
        except Exception as e:
            print(f"âŒ {key} íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue
        
        if "ERROR" in content.upper():
            total_errors += 1
            category = classify_error_type(content)
            category_counts[category] += 1

    # ê²°ê³¼ ì €ì¥ (S3)
    for category, count in category_counts.items():
        if count == 0: continue
        summary_data = {"date": date_str, "category": category, "error_count": count}
        s3_hook.load_string(
            string_data=json.dumps(summary_data, ensure_ascii=False),
            key=f"{STATS_ZONE}{category}/{date_str}_stats.json",
            bucket_name=BUCKET_NAME,
            replace=True
        )

    send_slack_report(date_str, total_logs, total_errors, category_counts)

# --- [5. DAG ì •ì˜] ---
with DAG(
    dag_id='cali_daily_stats_reporter_eks',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
    tags=['eks', 'analysis', 'slack']
) as dag:

    run_analysis = PythonOperator(
        task_id='process_logs_and_report',
        python_callable=daily_analysis_and_slack
    )