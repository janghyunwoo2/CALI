import os
import json
import requests
import gzip
import sys
import re
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.operators.python import PythonOperator
from airflow.models import Variable

sys.setrecursionlimit(3000)

# --- [1. ì„¤ì •] ---
BUCKET_NAME = os.getenv('S3_BACKUP_BUCKET') or "cali-logs-827913617635"
LANDING_ZONE = 'raw/'
STATS_ZONE = 'daily_stats/'
AWS_REGION = "ap-northeast-2" 

default_args = {
    'owner': 'cali_admin',
    'start_date': datetime(2026, 1, 28),
    'retries': 1,
    'retry_delay': timedelta(seconds=10),
}

# --- [2. ì—ëŸ¬ ë¶„ë¥˜ê¸° (ì œë„ˆë ˆì´í„° SVC ì´ë¦„ ê¸°ì¤€)] ---
def classify_error_by_svc(line):
    # ì œë„ˆë ˆì´í„° í¬ë§·: [LEVEL] TS svc/ver [TID]: MSG
    # ì •ê·œì‹ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì´ë¦„ë§Œ ì¶”ì¶œ (ì˜ˆ: payment-gateway)
    match = re.search(r'\]\s\d{4}-\d{2}-\d{2}T\S+\s([^/]+)/', line)
    if not match:
        return 'Other_Errors'
    
    svc_name = match.group(1).lower()
    
    if 'db-cache' in svc_name: return 'Database'
    if 'infra-eks' in svc_name: return 'Infra_EKS'
    if 'payment' in svc_name: return 'Payment'
    if 'auth-security' in svc_name: return 'Auth_Security'
    if 'biz-logic' in svc_name: return 'BusinessLogic'
    
    return 'Other_Errors'

# --- [3. ìŠ¬ë™ ì „ì†¡ í•¨ìˆ˜ (ì •í•©ì„± 100% ë²„ì „)] ---
def send_slack_report(date_str, total_files, total_errors, counts):
    webhook_url = Variable.get("SLACK_WEBHOOK_URL", default_var=os.getenv('SLACK_WEBHOOK_URL'))
    if not webhook_url: return

    all_categories = ["Database", "Infra_EKS", "Payment", "Auth_Security", "BusinessLogic", "Other_Errors"]
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë©”ì‹œì§€ ìƒì„±
    detail_msg = "\n".join([f"â€¢ {cat}: {counts.get(cat, 0):,}ê±´" for cat in all_categories])
    
    # ë¶„ë¥˜ëœ ì—ëŸ¬ í•©ê³„ ê³„ì‚° (ê²€ì¦ìš©)
    classified_sum = sum(counts.values())
    
    payload = {
        "text": f"ğŸ“… *{date_str} Cali ì‹œìŠ¤í…œ ë¶„ì„ ë¦¬í¬íŠ¸*",
        "attachments": [{
            "color": "#FF0000" if total_errors > 0 else "#36a64f",
            "blocks": [
                {
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": f"*ğŸ“Š ì „ì²´ í†µê³„*\nâ€¢ ë¶„ì„ íŒŒì¼: {total_files:,}ê°œ\nâ€¢ ì´ ì—ëŸ¬ ë¼ì¸: {total_errors:,}ê±´ (ë¶„ë¥˜ìœ¨: {(classified_sum/total_errors)*100:.1f}%)"}
                },
                {
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": f"*ğŸš¨ 5-Tier ë¶„ì„ í˜„í™©*\n{detail_msg}"}
                }
            ]
        }]
    }
    requests.post(webhook_url, json=payload)

# --- [4. ë©”ì¸ ë¶„ì„ ë¡œì§] ---
def daily_analysis_and_slack(**context):
    s3_hook = S3Hook(aws_conn_id=None, region_name=AWS_REGION)
    all_keys = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=LANDING_ZONE)
    clean_keys = [k for k in (all_keys or []) if k != LANDING_ZONE and not k.endswith('/')]
    
    counts = {cat: 0 for cat in ["Database", "Infra_EKS", "Payment", "Auth_Security", "BusinessLogic", "Other_Errors"]}
    total_files = 0
    total_errors = 0
    date_str = datetime.now().strftime('%Y-%m-%d')

    for key in clean_keys:
        raw_content = s3_hook.get_key(key, BUCKET_NAME).get()['Body'].read()
        total_files += 1
        try:
            content = gzip.decompress(raw_content).decode('utf-8') if raw_content.startswith(b'\x1f\x8b') else raw_content.decode('utf-8')
        except: continue
        
        for line in content.split('\n'):
            if not line.strip(): continue
            # ERROR, CRITICAL, WARN ëª¨ë‘ ì—ëŸ¬ë¡œ ì§‘ê³„ (ì œë„ˆë ˆì´í„° íŠ¹ì„± ë°˜ì˜)
            if any(lvl in line.upper() for lvl in ["ERROR", "CRITICAL", "WARN"]):
                total_errors += 1
                category = classify_error_by_svc(line)
                counts[category] += 1

    send_slack_report(date_str, total_files, total_errors, counts)

# --- [5. DAG ì •ì˜] ---
with DAG(
    dag_id='cali_daily_stats_reporter_eks',
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
    tags=['eks', 'analysis', 'slack']
) as dag:
    
    PythonOperator(task_id='process_logs_and_report', python_callable=daily_analysis_and_slack)