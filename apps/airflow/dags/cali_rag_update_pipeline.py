import os
import sys
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor 
from airflow.providers.amazon.aws.hooks.s3 import S3Hook 
from airflow.operators.python import PythonOperator 
from airflow.models import Variable

# EKS í™˜ê²½ ì¬ê·€ ì—ëŸ¬ ë°©ì–´
sys.setrecursionlimit(3000)

# --- [1. ìƒìˆ˜ ë° ì„¤ì •] ---
BUCKET_NAME = os.getenv('S3_BACKUP_BUCKET') or "cali-logs-827913617635"
COLLECTION_NAME = "cali_logs_test"
MILVUS_HOST = os.getenv('MILVUS_HOST') or "milvus.milvus.svc.cluster.local"
AWS_REGION = "ap-northeast-2" 

default_args = {
    'owner': 'cali_admin',
    'retries': 1,
    'retry_delay': timedelta(seconds=30),
}

# --- [2. ë©”ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í•¨ìˆ˜] ---
def process_cali_rag_logic(**context):
    try:
        from openai import OpenAI
        from pymilvus import connections, Collection, utility, FieldSchema, CollectionSchema, DataType
    except ImportError as e:
        print(f"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¸ì‹ ì‹¤íŒ¨: {e}")
        raise 

    api_key = os.getenv('OPENAI_API_KEY') or Variable.get("OPENAI_API_KEY", default_var=None)
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")

    s3_hook = S3Hook(aws_conn_id=None, region_name=AWS_REGION) 
    
    all_files = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix='solutions/')
    target_files = [f for f in (all_files or []) if f.endswith('.txt') and f != 'solutions/']
    
    if not target_files:
        print("ğŸ’¡ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“¡ Milvus ì—°ê²° ì‹œë„: {MILVUS_HOST}")
    connections.connect("default", host=MILVUS_HOST, port="19530")
    
    try:
        if not utility.has_collection(COLLECTION_NAME):
            fields = [
                FieldSchema(name="pk", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=1536), 
                FieldSchema(name="service", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="error_message", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="action", dtype=DataType.VARCHAR, max_length=100)
            ]
            col = Collection(COLLECTION_NAME, CollectionSchema(fields, "Cali RAG Base"))
            col.create_index("vector", {"metric_type": "L2", "index_type": "IVF_FLAT", "params": {"nlist": 128}})
        else:
            col = Collection(COLLECTION_NAME)
        
        col.load()
        ai_client = OpenAI(api_key=api_key)

        for target_file in target_files:
            print(f"ğŸ“‚ íŒŒì¼ ë¶„ì„ ì¤‘: {target_file}")
            content = s3_hook.read_key(target_file, BUCKET_NAME)
            
            if len(content.strip()) < 10: continue

            # OpenAI ì„ë² ë”© ìƒì„±
            response = ai_client.embeddings.create(
                model="text-embedding-3-small", 
                input=[content.replace("\n", " ")]
            )
            vector = response.data[0].embedding

            # Milvus ë°ì´í„° ì ì¬
            col.insert([[vector], ["cali_knowledge"], [content[:1024]], ["updated"]])
            col.flush()
            print(f"âœ… Milvus ì ì¬ ì„±ê³µ: {target_file}")

            # [í•µì‹¬ ìˆ˜ì •: íŒŒì¼ ì´ë™ ë° ì•ˆì „í•œ ì‚­ì œ]
            dest_key = target_file.replace('solutions/', 'processed/')
            
            # 1. íŒŒì¼ ë³µì‚¬ ì‹œë„
            s3_hook.copy_object(
                source_bucket_key=target_file, 
                dest_bucket_key=dest_key, 
                source_bucket_name=BUCKET_NAME, 
                dest_bucket_name=BUCKET_NAME
            )
            print(f"ğŸšš ë³µì‚¬ ì™„ë£Œ: {dest_key}")

            # 2. ì•ˆì „í•œ ì‚­ì œ ì‹œë„ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬ ë° ì˜ˆì™¸ ì²˜ë¦¬)
            try:
                # delete_objects ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸í˜•ìœ¼ë¡œ keys ì „ë‹¬
                s3_hook.delete_objects(bucket=BUCKET_NAME, keys=[target_file])
                print(f"ğŸ—‘ï¸ ì›ë³¸ ì‚­ì œ ì„±ê³µ: {target_file}")
            except Exception as delete_err:
                # ì‚­ì œ ê¶Œí•œì´ ì—†ë”ë¼ë„ ì ì¬ëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ì›Œë‹ë§Œ ë„ìš°ê³  ì¢…ë£Œ
                print(f"âš ï¸ ì‚­ì œ ì‹¤íŒ¨ (ê¶Œí•œ ë¶€ì¡± ê°€ëŠ¥ì„±): {delete_err}")

    finally:
        connections.disconnect("default")

# --- [3. ì—ì–´í”Œë¡œìš° DAG ì •ì˜] ---
with DAG(
    dag_id='cali_rag_update_pipeline',
    default_args=default_args,
    start_date=datetime(2026, 1, 27),
    schedule_interval=None,
    catchup=False,
    tags=['cali', 'rag', 'eks', 'milvus']
) as dag:

    wait_for_file = S3KeySensor(
        task_id='wait_for_solution_file',
        bucket_name=BUCKET_NAME,
        bucket_key='solutions/*.txt',
        wildcard_match=True,
        mode='reschedule',
        poke_interval=30,
        timeout=600,
        aws_conn_id=None
    )

    run_main_logic = PythonOperator(
        task_id='run_cali_rag_ingestion',
        python_callable=process_cali_rag_logic
    )

    wait_for_file >> run_main_logic