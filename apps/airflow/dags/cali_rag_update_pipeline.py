import os
import sys
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor 
from airflow.providers.amazon.aws.hooks.s3 import S3Hook 
from airflow.operators.python import PythonOperator 
from airflow.models import Variable

# EKS í™˜ê²½ ì¬ê·€ ì—ëŸ¬ ë°©ì–´
sys.setrecursionlimit(3000)

# --- [1. ìƒìˆ˜ ë° ì„¤ì •] ---
BUCKET_NAME = os.getenv('S3_BACKUP_BUCKET') or "cali-logs-827913617635"
COLLECTION_NAME = "cali_logs_test"
MILVUS_HOST = os.getenv('MILVUS_HOST') or "milvus.milvus.svc.cluster.local"
AWS_REGION = "ap-northeast-2" 

default_args = {
    'owner': 'cali_admin',
    'retries': 1,
    'retry_delay': timedelta(seconds=30),
}

# --- [2. ë©”ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í•¨ìˆ˜] ---
def process_cali_rag_logic(**context):
    try:
        from openai import OpenAI
        from pymilvus import connections, Collection, utility, FieldSchema, CollectionSchema, DataType
    except ImportError as e:
        print(f"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¸ì‹ ì‹¤íŒ¨: {e}")
        raise 

    api_key = os.getenv('OPENAI_API_KEY') or Variable.get("OPENAI_API_KEY", default_var=None)
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")

    # S3Hookì€ region_nameì„ ì§ì ‘ ë°›ì„ ìˆ˜ ìˆìŒ
    s3_hook = S3Hook(aws_conn_id=None, region_name=AWS_REGION) 
    
    all_files = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix='solutions/')
    target_files = [f for f in (all_files or []) if f.endswith('.txt') and f != 'solutions/']
    
    if not target_files:
        print("ğŸ’¡ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“¡ Milvus ì—°ê²° ì‹œë„: {MILVUS_HOST}")
    connections.connect("default", host=MILVUS_HOST, port="19530")
    
    try:
        if not utility.has_collection(COLLECTION_NAME):
            fields = [
                FieldSchema(name="pk", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=1536), 
                FieldSchema(name="service", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="error_message", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="action", dtype=DataType.VARCHAR, max_length=100)
            ]
            schema = CollectionSchema(fields, "Cali RAG Knowledge Base")
            col = Collection(COLLECTION_NAME, schema)
            col.create_index("vector", {"metric_type": "L2", "index_type": "IVF_FLAT", "params": {"nlist": 128}})
        else:
            col = Collection(COLLECTION_NAME)
        
        col.load()
        ai_client = OpenAI(api_key=api_key)

        for target_file in target_files:
            print(f"ğŸ“‚ íŒŒì¼ ë¶„ì„ ì¤‘: {target_file}")
            content = s3_hook.read_key(target_file, BUCKET_NAME)
            
            if len(content.strip()) < 10: continue

            response = ai_client.embeddings.create(
                model="text-embedding-3-small", 
                input=[content.replace("\n", " ")]
            )
            vector = response.data[0].embedding

            col.insert([[vector], ["cali_knowledge"], [content[:1024]], ["updated"]])
            col.flush()

            # [íŒŒì¼ ì´ë™ ë¡œì§]
            dest_key = target_file.replace('solutions/', 'processed/')
            s3_hook.copy_object(
                source_bucket_key=target_file, 
                dest_bucket_key=dest_key, 
                source_bucket_name=BUCKET_NAME, 
                dest_bucket_name=BUCKET_NAME
            )
            s3_hook.delete_objects(bucket=BUCKET_NAME, keys=target_file)
            print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ ë° ì´ë™: {target_file} -> {dest_key}")

    finally:
        connections.disconnect("default")

# --- [3. ì—ì–´í”Œë¡œìš° DAG ì •ì˜] ---
with DAG(
    dag_id='cali_rag_update_pipeline',
    default_args=default_args,
    start_date=datetime(2026, 1, 27),
    schedule_interval=None,
    catchup=False,
    tags=['cali', 'rag', 'eks', 'milvus']
) as dag:

    # ğŸŒŸ [ì—ëŸ¬ ìˆ˜ì • í•µì‹¬ í¬ì¸íŠ¸] 
    # S3KeySensorëŠ” region_nameì„ ì§ì ‘ ë°›ì§€ ì•Šê³  hook_paramsì— ë„£ì–´ì•¼ í•¨
    wait_for_file = S3KeySensor(
        task_id='wait_for_solution_file',
        bucket_name=BUCKET_NAME,
        bucket_key='solutions/*.txt',
        wildcard_match=True,
        mode='reschedule',
        poke_interval=30,
        timeout=600,
        aws_conn_id=None,
        verify=False,
        hook_params={
            "region_name": AWS_REGION  # <-- ìš”ë ‡ê²Œ ì£¼ë¨¸ë‹ˆì— ë‹´ì•„ì¤˜ì•¼ í•¨!
        }
    )

    run_main_logic = PythonOperator(
        task_id='run_cali_rag_ingestion',
        python_callable=process_cali_rag_logic
    )

    wait_for_file >> run_main_logic