import os
import sys
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor 
from airflow.providers.amazon.aws.hooks.s3 import S3Hook 
from airflow.operators.python import PythonOperator 
from airflow.models import Variable

# EKS í™˜ê²½ì—ì„œ Airflowì˜ ë‚´ë¶€ ì¬ê·€ í˜¸ì¶œ ë¬¸ì œë¥¼ ë°©ì–´í•˜ê¸° ìœ„í•œ ì„¤ì •
sys.setrecursionlimit(3000)

# --- [1. ìƒìˆ˜ ë° ì„¤ì •] ---
# EKS í¬ë“œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìˆ˜ë™ ì„¤ì •
BUCKET_NAME = os.getenv('S3_BACKUP_BUCKET') or "cali-logs-827913617635"
COLLECTION_NAME = "cali_logs_test"
MILVUS_HOST = os.getenv('MILVUS_HOST') or "milvus-standalone.milvus.svc.cluster.local"
AWS_REGION = "ap-northeast-2" 

default_args = {
    'owner': 'cali_admin',
    'retries': 1,
    'retry_delay': timedelta(seconds=30),
}

# --- [2. ë©”ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í•¨ìˆ˜] ---
def process_cali_rag_logic(**context):
    try:
        from openai import OpenAI
        from pymilvus import connections, Collection, utility, FieldSchema, CollectionSchema, DataType
    except ImportError as e:
        print(f"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¸ì‹ ì‹¤íŒ¨: {e}")
        raise 

    # EKSì—ì„œëŠ” Variableë³´ë‹¤ëŠ” í™˜ê²½ë³€ìˆ˜(Env)ê°€ ì¬ê·€ ì—ëŸ¬ ë°©ì§€ì— ë” ìœ ë¦¬í•¨
    api_key = os.getenv('OPENAI_API_KEY') or Variable.get("OPENAI_API_KEY", default_var=None)
    if not api_key:
        raise ValueError("OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. ì—ì–´í”Œë¡œìš° Variableì´ë‚˜ Envë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # [í•µì‹¬] EKS IRSA(IAM Role for Service Accounts)ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ conn_id=None ì„¤ì •
    s3_hook = S3Hook(aws_conn_id=None, region_name=AWS_REGION) 
    
    # 1. solutions/ í´ë” ë‚´ íŒŒì¼ ê°ì§€ (ì„¼ì„œê°€ ì¡ì•„ì¤¬ì§€ë§Œ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ì—…)
    all_files = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix='solutions/')
    target_files = [f for f in (all_files or []) if f.endswith('.txt') and f != 'solutions/']
    
    if not target_files:
        print("ğŸ’¡ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì„¼ì„œ ì˜¤ì‘ë™ í˜¹ì€ ì´ë¯¸ ì²˜ë¦¬ë¨.")
        return

    # Milvus ì—°ê²° (EKS Cluster ë‚´ë¶€ DNS ì‚¬ìš©)
    print(f"ğŸ“¡ Milvus ì—°ê²° ì‹œë„: {MILVUS_HOST}")
    connections.connect("default", host=MILVUS_HOST, port="19530")
    
    try:
        # ì»¬ë ‰ì…˜ ë° ì¸ë±ìŠ¤ ìƒì„± ë¡œì§ (í˜•ì˜ ìŠ¤í‚¤ë§ˆ ìœ ì§€)
        if not utility.has_collection(COLLECTION_NAME):
            fields = [
                FieldSchema(name="pk", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=1536), 
                FieldSchema(name="service", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="error_message", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="action", dtype=DataType.VARCHAR, max_length=100)
            ]
            schema = CollectionSchema(fields, "Cali RAG Knowledge Base")
            col = Collection(COLLECTION_NAME, schema)
            col.create_index("vector", {"metric_type": "L2", "index_type": "IVF_FLAT", "params": {"nlist": 128}})
        else:
            col = Collection(COLLECTION_NAME)
        
        col.load()
        ai_client = OpenAI(api_key=api_key)

        for target_file in target_files:
            print(f"ğŸ“‚ íŒŒì¼ ë¶„ì„ ì¤‘: {target_file}")
            content = s3_hook.read_key(target_file, BUCKET_NAME)
            
            if len(content.strip()) < 10: 
                print(f"âš ï¸ {target_file} ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue

            # OpenAI ì„ë² ë”© ìƒì„± (text-embedding-3-small)
            response = ai_client.embeddings.create(
                model="text-embedding-3-small", 
                input=[content.replace("\n", " ")]
            )
            vector = response.data[0].embedding

            # Milvus ë°ì´í„° ì‚½ì…
            col.insert([[vector], ["cali_knowledge"], [content[:1024]], ["updated"]])
            col.flush()
            print(f"âœ… Milvus ì ì¬ ì„±ê³µ: {target_file}")

            # [íŒŒì¼ ì´ë™] ì‘ì—… ì™„ë£Œ í›„ processed/ í´ë”ë¡œ ì´ë™ (S3 ì´ë™ ì „ëµ)
            dest_key = target_file.replace('solutions/', 'processed/')
            s3_hook.copy_object(
                source_bucket_key=target_file, 
                dest_bucket_key=dest_key, 
                source_bucket_name=BUCKET_NAME, 
                dest_bucket_name=BUCKET_NAME
            )
            s3_hook.delete_objects(bucket=BUCKET_NAME, keys=target_file)
            print(f"ğŸšš íŒŒì¼ ì•„ì¹´ì´ë¹™ ì™„ë£Œ: {target_file} -> {dest_key}")

    finally:
        connections.disconnect("default")

# --- [3. ì—ì–´í”Œë¡œìš° DAG ì •ì˜] ---
with DAG(
    dag_id='cali_rag_update_pipeline',
    default_args=default_args,
    start_date=datetime(2026, 1, 27),
    schedule_interval=None, # ì‹œì—° ì‹œ Triggerë¡œ ì‹¤í–‰
    catchup=False,
    tags=['cali', 'rag', 'eks', 'milvus']
) as dag:

    # [í•µì‹¬] ì„¼ì„œì—ì„œë„ conn_id=Noneê³¼ regionì„ ëª…ì‹œí•´ì•¼ ì¬ê·€ ì—ëŸ¬ ì•ˆ í„°ì§
    wait_for_file = S3KeySensor(
        task_id='wait_for_solution_file',
        bucket_name=BUCKET_NAME,
        bucket_key='solutions/*.txt',
        wildcard_match=True,
        mode='reschedule', # EKS ë¦¬ì†ŒìŠ¤ ì ˆì•½ì„ ìœ„í•´ reschedule ëª¨ë“œ ì¶”ì²œ
        poke_interval=30,
        timeout=600,
        aws_conn_id=None, # ì¤‘ìš”!
        region_name=AWS_REGION # ì¤‘ìš”!
    )

    run_main_logic = PythonOperator(
        task_id='run_cali_rag_ingestion',
        python_callable=process_cali_rag_logic
    )

    wait_for_file >> run_main_logic